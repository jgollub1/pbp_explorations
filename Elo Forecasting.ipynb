{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tny_id</th>\n",
       "      <th>tny_name</th>\n",
       "      <th>surface</th>\n",
       "      <th>tny_date</th>\n",
       "      <th>match_year</th>\n",
       "      <th>match_month</th>\n",
       "      <th>p0_name</th>\n",
       "      <th>p1_name</th>\n",
       "      <th>p0_elo</th>\n",
       "      <th>p1_elo</th>\n",
       "      <th>...</th>\n",
       "      <th>match_prob_sf_kls_JS</th>\n",
       "      <th>match_prob_adj_kls</th>\n",
       "      <th>match_prob_adj_kls_JS</th>\n",
       "      <th>elo_prob</th>\n",
       "      <th>elo_prob_538</th>\n",
       "      <th>sf_elo_prob</th>\n",
       "      <th>sf_elo_prob_538</th>\n",
       "      <th>s_total</th>\n",
       "      <th>p0_s_kls_elo</th>\n",
       "      <th>p1_s_kls_elo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-339</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>Hard</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Arnaud Clement</td>\n",
       "      <td>Thomas Enqvist</td>\n",
       "      <td>1675.243826</td>\n",
       "      <td>1921.802173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354126</td>\n",
       "      <td>0.223227</td>\n",
       "      <td>0.224541</td>\n",
       "      <td>0.194771</td>\n",
       "      <td>0.204867</td>\n",
       "      <td>0.211304</td>\n",
       "      <td>0.233946</td>\n",
       "      <td>1.238109</td>\n",
       "      <td>0.585200</td>\n",
       "      <td>0.652909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-339</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>Hard</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Roger Federer</td>\n",
       "      <td>Jens Knippschild</td>\n",
       "      <td>1664.533683</td>\n",
       "      <td>1697.781488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476991</td>\n",
       "      <td>0.431598</td>\n",
       "      <td>0.432470</td>\n",
       "      <td>0.452298</td>\n",
       "      <td>0.518498</td>\n",
       "      <td>0.546058</td>\n",
       "      <td>0.594731</td>\n",
       "      <td>1.287473</td>\n",
       "      <td>0.638864</td>\n",
       "      <td>0.648608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tny_id  tny_name surface    tny_date  match_year  match_month  \\\n",
       "0  2000-339  Adelaide    Hard  2000-01-03        2000            1   \n",
       "1  2000-339  Adelaide    Hard  2000-01-03        2000            1   \n",
       "\n",
       "          p0_name           p1_name       p0_elo       p1_elo      ...       \\\n",
       "0  Arnaud Clement    Thomas Enqvist  1675.243826  1921.802173      ...        \n",
       "1   Roger Federer  Jens Knippschild  1664.533683  1697.781488      ...        \n",
       "\n",
       "   match_prob_sf_kls_JS  match_prob_adj_kls  match_prob_adj_kls_JS  elo_prob  \\\n",
       "0              0.354126            0.223227               0.224541  0.194771   \n",
       "1              0.476991            0.431598               0.432470  0.452298   \n",
       "\n",
       "   elo_prob_538  sf_elo_prob  sf_elo_prob_538   s_total  p0_s_kls_elo  \\\n",
       "0      0.204867     0.211304         0.233946  1.238109      0.585200   \n",
       "1      0.518498     0.546058         0.594731  1.287473      0.638864   \n",
       "\n",
       "   p1_s_kls_elo  \n",
       "0      0.652909  \n",
       "1      0.648608  \n",
       "\n",
       "[2 rows x 95 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/jacobgollub/Desktop/college/research/pbp_explorations/scripts/sackmann')\n",
    "import tennisGameProbability,tennisMatchProbability,tennisSetProbability,tennisTiebreakProbability\n",
    "from tennisMatchProbability import matchProb\n",
    "from helper_functions import *\n",
    "from data_functions import generate_elo_induced_s\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "FILE_NAME = 'elo_atp_matches_all_10_2.csv'\n",
    "# can test this on our subset of 10,000 matches as well as all matches in the database:\n",
    "df = pd.read_csv('../my_data/'+FILE_NAME)\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "# currently looking at 2014 tour-level matches, excluding Davis Cup\n",
    "#df = df[df['match_year']==2014].reset_index(drop=True)\n",
    "#df = df[df['match_year'].isin([2011,2012,2013,2014,2015])]\n",
    "df = df[df['tny_name']!='Davis Cup'].reset_index(drop=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9705, 8540)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub[sub['elo_diff']>0]),len(sub[sub['elo_diff']<0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6899934239368698"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = df[df['match_year']>=2010]\n",
    "s = (len(sub[(sub['elo_diff']>0) & (sub['winner'])])+len(sub[(sub['elo_diff']<0) & (sub['winner']==0)]))\n",
    "s /= float(len(sub))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76071848681709175"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "x = pearsonr(df['match_prob_adj_kls_JS'],df['elo_prob'])[0]\n",
    "x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df = df[df['match_year']==2014]\n",
    "# print 'elo baseline: ',  sum((df['elo_diff']>0) == df['winner'])/float(len(df))\n",
    "# print log_loss(df['winner'],[(1+10**(diff/-400.))**-1 for diff in df['elo_diff']])\n",
    "# print log_loss(df['winner'],[(1+10**(diff/-400.))**-1 for diff in df['sf_elo_diff']])\n",
    "# print 'surface elo baseline: ', sum((df['sf_elo_diff']>0) == df['winner'])/float(len(df))\n",
    "# print 'elo 538 baseline: ',  sum((df['elo_diff_538']>0) == df['winner'])/float(len(df))\n",
    "# print log_loss(df['winner'],[(1+10**(diff/-400.))**-1 for diff in df['elo_diff_538']])\n",
    "# print log_loss(df['winner'],[(1+10**(diff/-400.))**-1 for diff in df['sf_elo_diff_538']])\n",
    "# print 'surface elo 538 baseline: ', sum((df['sf_elo_diff_538']>0) == df['winner'])/float(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_prob_kls\n",
      "accuracy:  0.649020004041\n",
      "loss:  0.641058120068\n",
      "match_prob_kls_JS\n",
      "accuracy:  0.65306122449\n",
      "loss:  0.612918235363\n",
      "match_prob_sf_kls\n",
      "accuracy:  0.631238634067\n",
      "loss:  0.709943609271\n",
      "match_prob_sf_kls_JS\n",
      "accuracy:  0.632451000202\n",
      "loss:  0.638863099676\n",
      "match_prob_adj_kls\n",
      "accuracy:  0.679531218428\n",
      "loss:  0.620431497622\n",
      "match_prob_adj_kls_JS\n",
      "accuracy:  0.67973327945\n",
      "loss:  0.608389757952\n",
      "lm columns:  ['elo_diff']\n",
      "accuracy:  0.690644574662\n",
      "loss:  0.578209904735\n",
      "lm columns:  ['sf_elo_diff']\n",
      "accuracy:  0.683774499899\n",
      "loss:  0.584881254079\n",
      "lm columns:  ['elo_diff_538']\n",
      "accuracy:  0.692463123863\n",
      "loss:  0.577909913273\n",
      "lm columns:  ['sf_elo_diff_538']\n",
      "accuracy:  0.690846635684\n",
      "loss:  0.582051444119\n",
      "lm columns:  ['elo_diff', 'sf_elo_diff']\n",
      "accuracy:  0.696908466357\n",
      "loss:  0.574712255606\n",
      "lm columns:  ['elo_diff_538', 'sf_elo_diff_538']\n",
      "accuracy:  0.693069306931\n",
      "loss:  0.573451613361\n"
     ]
    }
   ],
   "source": [
    "cols = [['elo_diff'],['sf_elo_diff'],['elo_diff_538'],['sf_elo_diff_538'],\n",
    "        ['elo_diff','sf_elo_diff'],['elo_diff_538','sf_elo_diff_538']]\n",
    "#         ['elo_diff','sf_elo_diff','match_z_kls'],\\\n",
    "#         ['elo_diff_538','sf_elo_diff_538','match_z_kls']]\n",
    "probs = [u'match_prob_kls',u'match_prob_kls_JS', u'match_prob_sf_kls', u'match_prob_sf_kls_JS',\n",
    "        'match_prob_adj_kls','match_prob_adj_kls_JS']\n",
    "n_splits = 5\n",
    "train_df,test_df = df[df['match_year']!=2014],df[df['match_year'].isin([2014,2015])]\n",
    "#validate_results(df,probs=probs,lm_columns=cols,n_splits=n_splits)\n",
    "y_logit_probs = test_results(train_df,test_df,probs=probs,lm_columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cols = [['elo_diff'],['sf_elo_diff'],['elo_diff_538'],['sf_elo_diff_538'],\n",
    "#         ['elo_diff','sf_elo_diff'],['elo_diff_538','sf_elo_diff_538'],\\\n",
    "#         ['elo_diff','sf_elo_diff','match_z_kls'],\\\n",
    "#         ['elo_diff_538','sf_elo_diff_538','match_z_kls']]\n",
    "# probs = [u'match_prob_kls',u'match_prob_kls_JS', u'match_prob_sf_kls', u'match_prob_sf_kls_JS',\n",
    "#         'match_prob_adj_kls','match_prob_adj_kls_JS']\n",
    "# probs = [u'match_prob_kls',u'match_prob_kls_JS', u'match_prob_sf_kls', u'match_prob_sf_kls_JS',\n",
    "#         'match_prob_adj_kls','match_prob_adj_kls_JS']\n",
    "# n_splits = 5\n",
    "# validate_results(sub_df,probs=probs,lm_columns=cols,n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.to_csv('../my_data/elo_atp_matches_21st_century_9_12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try out different weightings of normalized surface serve stats...\n",
    "# probs = []\n",
    "# for p in [.1,.2,.3,.4,.5,.6,.7,.8,.9]:\n",
    "#     wgt_s0 = (1-p)*df['p0_s_pct_JS'] + p*df['p0_sf_s_pct_JS']\n",
    "#     wgt_s1 = (1-p)*df['p1_s_pct_JS'] + p*df['p1_sf_s_pct_JS']\n",
    "#     wgt_r0 = (1-p)*df['p0_r_pct_JS'] + p*df['p0_sf_r_pct_JS']\n",
    "#     wgt_r1 = (1-p)*df['p1_r_pct_JS'] + p*df['p1_sf_r_pct_JS']\n",
    "#     df['p0_s_wgt'+str(p)] = df['tny_stats']+(wgt_s0-df['avg_52_s']) - (wgt_r1-df['avg_52_r'])\n",
    "#     df['p1_s_wgt'+str(p)] = df['tny_stats']+(wgt_s1-df['avg_52_s']) - (wgt_r0-df['avg_52_r'])\n",
    "    \n",
    "#     df['match_prob_wgt_'+str(p)] = [matchProb(row['p0_s_wgt'+str(p)],1-row['p1_s_wgt'+str(p)]) \\\n",
    "#                                     for i,row in df.iterrows()]\n",
    "#     probs.append('match_prob_wgt_'+str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-time fix to transfer over the elo_diff's\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# df_og = pd.read_csv('../my_data/elo_pbp_with_surface_8_22.csv')\n",
    "# del df_og['Unnamed: 0']\n",
    "# df = pd.read_csv('../my_data/feature_df_pbp3_8_23_alphas.csv')\n",
    "# del df['Unnamed: 0']\n",
    "\n",
    "# elo_dict = dict(zip(range(len(df_og)),df_og['elo_diff']))\n",
    "# s_elo_dict = dict(zip(range(len(df_og)),df_og['s_elo_diff']))\n",
    "\n",
    "# df['elo_diff'] = [elo_dict[m_id] for m_id in df['match_id']]\n",
    "# df['s_elo_diff'] = [s_elo_dict[m_id] for m_id in df['match_id']]\n",
    "# df.to_csv('../my_data/feature_df_pbp3_8_23_alphas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import linear_model\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "# n_splits = 5\n",
    "# kfold = KFold(n_splits=n_splits,shuffle=True)\n",
    "# scores = np.zeros([3,2,n_splits]);i=0\n",
    "# cols = ['elo_diff','s_elo_diff']\n",
    "# for train_ind,test_ind in kfold.split(df):\n",
    "#     lm = linear_model.LogisticRegression(fit_intercept = True)\n",
    "#     train_df,test_df = df.loc[train_ind],df.loc[test_ind]\n",
    "#     lm.fit(train_df[cols].values.reshape([len(train_df),len(cols)]),train_df['winner'])\n",
    "#     y_preds = lm.predict(test_df[cols].values.reshape([len(test_df),len(cols)]))\n",
    "#     y_preds2 = test_df['match_prob_kls']>.5\n",
    "#     y_preds3 = test_df['match_prob_kls_JS']>.5\n",
    "#     y_probs = lm.predict_proba(test_df[cols].values.reshape([len(test_df),len(cols)]))\n",
    "#     scores[0][0][i]=accuracy_score(test_df['winner'],y_preds)\n",
    "#     scores[0][1][i]=log_loss(test_df['winner'],y_probs,labels=[0,1])\n",
    "#     scores[1][0][i]=accuracy_score(test_df['winner'],y_preds2)\n",
    "#     scores[1][1][i]=log_loss(test_df['winner'],test_df['match_prob_kls'],labels=[0,1])\n",
    "#     scores[2][0][i]=accuracy_score(test_df['winner'],y_preds3)\n",
    "#     scores[2][1][i]=log_loss(test_df['winner'],test_df['match_prob_kls_JS'],labels=[0,1])\n",
    "\n",
    "    \n",
    "#     i+=1\n",
    "# print '% s_elo used in lm fit: ',lm.coef_[0][1]/(lm.coef_[0][0]+lm.coef_[0][1])\n",
    "# print 'accuracy: ', np.mean(scores[0][0])\n",
    "# print 'loss: ', np.mean(scores[0][1])\n",
    "\n",
    "# print 'kls probabilities'\n",
    "# print 'accuracy: ', np.mean(scores[1][0])\n",
    "# print 'loss: ', np.mean(scores[1][1])\n",
    "\n",
    "# print 'kls JS probabilities'\n",
    "# print 'accuracy: ', np.mean(scores[2][0])\n",
    "# print 'loss: ', np.mean(scores[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import linear_model\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "# # cols is a list of all column sets to test; compare with kls pre-match forecasts\n",
    "# def validate_results(df,columns,n_splits=5):\n",
    "#     kfold = KFold(n_splits=n_splits,shuffle=True)\n",
    "#     scores = np.zeros([len(columns)+2,2,n_splits]);i=0\n",
    "#     for train_ind,test_ind in kfold.split(df):\n",
    "#         lm = linear_model.LogisticRegression(fit_intercept = True)\n",
    "#         train_df,test_df = df.loc[train_ind],df.loc[test_ind]\n",
    "        \n",
    "#         for k,cols in enumerate(columns):\n",
    "#             lm.fit(train_df[cols].values.reshape([len(train_df),len(cols)]),train_df['winner'])\n",
    "#             y_preds = lm.predict(test_df[cols].values.reshape([len(test_df),len(cols)]))\n",
    "#             y_probs = lm.predict_proba(test_df[cols].values.reshape([len(test_df),len(cols)]))\n",
    "#             scores[k][0][i]=accuracy_score(test_df['winner'],y_preds)\n",
    "#             scores[k][1][i]=log_loss(test_df['winner'],y_probs,labels=[0,1])\n",
    "        \n",
    "#         y_preds2 = test_df['match_prob_kls']>.5\n",
    "#         y_preds3 = test_df['match_prob_kls_JS']>.5\n",
    "#         scores[len(columns)][0][i]=accuracy_score(test_df['winner'],y_preds2)\n",
    "#         scores[len(columns)][1][i]=log_loss(test_df['winner'],test_df['match_prob_kls'],labels=[0,1])\n",
    "#         scores[len(columns)+1][0][i]=accuracy_score(test_df['winner'],y_preds3)\n",
    "#         scores[len(columns)+1][1][i]=log_loss(test_df['winner'],test_df['match_prob_kls_JS'],labels=[0,1])\n",
    "#         i+=1\n",
    "    \n",
    "#     for i,cols in enumerate(columns):\n",
    "#         print 'columns: ',cols\n",
    "#         #print '% s_elo used in lm fit: ',lm.coef_[0][1]/(lm.coef_[0][0]+lm.coef_[0][1])\n",
    "#         print 'accuracy: ', np.mean(scores[i][0])\n",
    "#         print 'loss: ', np.mean(scores[i][1])\n",
    "    \n",
    "#     print 'kls probabilities'\n",
    "#     print 'accuracy: ', np.mean(scores[len(columns)][0])\n",
    "#     print 'loss: ', np.mean(scores[len(columns)][1])\n",
    "\n",
    "#     print 'kls JS probabilities'\n",
    "#     print 'accuracy: ', np.mean(scores[len(columns)+1][0])\n",
    "#     print 'loss: ', np.mean(scores[len(columns)+1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import linear_model\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import log_loss,accuracy_score\n",
    "# # cols is a list of column sets for logistic regression; \n",
    "# # probs are model-specific probabilities\n",
    "# def validate_results(df,probs,lm_columns,n_splits=5):\n",
    "#     kfold = KFold(n_splits=n_splits,shuffle=True)\n",
    "#     scores = np.zeros([len(lm_columns)+len(probs),2,n_splits]);i=0\n",
    "#     for train_ind,test_ind in kfold.split(df):\n",
    "#         lm = linear_model.LogisticRegression(fit_intercept = True)\n",
    "#         train_df,test_df = df.loc[train_ind],df.loc[test_ind]\n",
    "        \n",
    "#         for j,prob_col in enumerate(probs):\n",
    "#             y_preds = test_df[prob_col]>.5\n",
    "#             scores[j][0][i]=accuracy_score(test_df['winner'],y_preds)\n",
    "#             scores[j][1][i]=log_loss(test_df['winner'],test_df[prob_col],labels=[0,1])\n",
    "        \n",
    "#         for k,cols in enumerate(lm_columns):\n",
    "#             lm.fit(train_df[cols].values.reshape([len(train_df),len(cols)]),train_df['winner'])\n",
    "#             y_preds = lm.predict(test_df[cols].values.reshape([len(test_df),len(cols)]))\n",
    "#             y_probs = lm.predict_proba(test_df[cols].values.reshape([len(test_df),len(cols)]))\n",
    "#             scores[len(probs)+k][0][i]=accuracy_score(test_df['winner'],y_preds)\n",
    "#             scores[len(probs)+k][1][i]=log_loss(test_df['winner'],y_probs,labels=[0,1])\n",
    "#         i+=1\n",
    "\n",
    "#     for j,prob_col in enumerate(probs):\n",
    "#         print prob_col\n",
    "#         print 'accuracy: ', np.mean(scores[j][0])\n",
    "#         print 'loss: ', np.mean(scores[j][1])\n",
    "    \n",
    "#     for i,cols in enumerate(lm_columns):\n",
    "#         print 'lm columns: ',cols\n",
    "#         print 'accuracy: ', np.mean(scores[len(probs)+i][0])\n",
    "#         print 'loss: ', np.mean(scores[len(probs)+i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# current = (0,0)\n",
    "# for weight in np.arange(11)*.1:\n",
    "#     accuracy = sum(((1-weight)*df['elo_diff_538']+weight*df['sf_elo_diff_538']>0) == df['winner'])/float(len(df))\n",
    "#     if accuracy > current[1]:\n",
    "#         current=weight,accuracy\n",
    "# print 'best s_elo weight: ',current[0],'accuracy =',current[1]\n",
    "\n",
    "# sub_df = df[(df['match_prob_adj_kls_JS']>.1) & (df['match_prob_adj_kls_JS']<.9)]\n",
    "# sub_df = sub_df.reset_index(drop=True)\n",
    "# ((df['elo_prob_538']>df['elo_prob_538']) & (df['winner']==1))\n",
    "\n",
    "# df['best_prob'] = np.zeros(len(df))\n",
    "# elo_ind = ((df['elo_prob_538']>df['sf_elo_prob_538']) & (df['winner']==1) | \n",
    "#             (df['elo_prob_538']<df['sf_elo_prob_538']) & (df['winner']==0))\n",
    "# df['best_prob'].loc[elo_ind] = df['elo_prob_538'].loc[elo_ind]\n",
    "# df['best_prob'].loc[~elo_ind] = df['sf_elo_prob_538'].loc[~elo_ind]\n",
    "\n",
    "# log_loss(df['winner'],df['best_prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from scipy.stats import norm\n",
    "# for col in ['match_prob_kls','match_prob_kls_JS','match_prob_adj_kls_JS']:\n",
    "#     df[col.replace('prob','z')] = norm.ppf(df[col])\n",
    "# print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6646747648902821"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "atp_year_list = []\n",
    "for i in xrange(2010,2018):\n",
    "    atp_year_list.append(pd.read_csv(\"../tennis_data/\"+TOUR+\"/\"+TOUR+\"_matches_{0}.csv\".format(i)))\n",
    "df = pd.concat(atp_year_list, ignore_index = True)\n",
    "\n",
    "sub = atp_all_matches\n",
    "higher_ind = sub['winner_rank']<sub['loser_rank']\n",
    "lower_ind = sub['winner_rank']>sub['loser_rank']\n",
    "\n",
    "len(sub[higher_ind])/float(len(sub))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
